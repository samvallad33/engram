# ENGRAM DEMO VIDEO — DIRECTOR'S CUT (3:00)

## [0:00 — THE HOOK & THE STAKES] (20 seconds)

Visual: Screen is pitch black. A heartbeat pulses in the audio. Stark white text slams onto the screen: **10%.**

AUDIO: Ten percent. That is the percentage of patient deaths currently linked to diagnostic errors.

Visual: The text shatters into fragments.

AUDIO: By 2030, the world will be short ten million healthcare workers. Medical students are burning out, drowning in static textbooks, and forgetting critical visual patterns within weeks. We can't build medical schools fast enough.

Visual: Quick, rhythmic cuts of traditional studying, flashing to lines of code, then booming into the sleek, dark-mode ENGRAM UI.

AUDIO: So, I hacked how the human brain learns instead.

## [0:20 — THE MECH SUIT] (40 seconds)

Visual: The user clicks "Start Session." A Chest X-Ray loads instantly.

AUDIO: Meet ENGRAM. It is not a chatbot. It is a pedagogical exoskeleton powered by five state-of-the-art Google foundation models, running entirely offline, on a single GPU.

Visual: User clicks the microphone, speaks, and text perfectly appears.

AUDIO: Real doctors dictate. So ENGRAM uses Google's MedASR to catch audio with 58 percent fewer errors than Whisper.

Visual: The user hits "Submit." MedGemma instantly draws high-fidelity bounding boxes on the X-Ray, accompanied by structured JSON teaching data.

AUDIO: MedGemma 1.5 serves as the core reasoning engine—analyzing the image, grading the response, and drawing the exact bounding boxes of what the student missed. But it doesn't stop there.

## [1:00 — THE COGNITIVE INTERCEPTION] (45 seconds)

Visual: A visualization of the FSRS-6 algorithm power-law curve.

AUDIO: ENGRAM is wired directly into FSRS-6, a 21-parameter algorithm that mathematically models cognitive decay. It knows you haven't seen an Atelectasis in six days. It knows your retention just dropped to 68 percent. It intervenes at the exact moment you are about to forget.

Visual: Rapid, rhythmic UI cuts. 3 seconds per mode. High energy.

AUDIO: We built six training modes to break human cognitive bias:

Satisfaction of Search: Found one tumor but missed the second? ENGRAM mathematically penalizes your review interval.

Gestalt Mode: A three-second flash tests your intuitive recognition before your analytical brain kicks in.

Contrastive Pairs: MedSigLIP retrieves visually identical but diagnostically different cases and forces you to spot the microscopic differences side-by-side.

Listen Then Look: HeAR plays synthetic bioacoustic lung sounds. Hear the crackles, predict the X-ray, then see the truth.

## [1:45 — THE PARADIGM SHIFT / THE NOVEL TASK] (45 seconds)

Visual: The UI zooms out to reveal the 'Biological-Digital Bridge' architecture diagram. FSRS signals (D=8.2) flowing directly into QLoRA weights.

AUDIO: But here is the invention. Here is why ENGRAM is entirely unique.

Visual: Zoom in on the LoRA fine-tuning code snippet as it runs.

AUDIO: As students review cases, the system tracks their failure rates. We take those human difficulty scores and use them as literal sampling weights for MedGemma's LoRA fine-tuning.

Visual: A glowing heatmap of blind spots appears, shifting and adapting.

AUDIO: Every other curriculum system uses model-internal math. ENGRAM is the first system to use human cognitive failure rates to rewrite a Vision-Language Model's neural weights. The system mathematically forces the AI to become a better teacher in the exact areas humans are worst at remembering. The ground truth remains clinically verified; the AI just learns to teach it louder.

## [2:30 — THE MIC DROP] (30 seconds)

Visual: The UI fades out. Crisp white text appears on black, one line at a time.

> 2,200 lines of Python.
> 5 Google Foundation Models.
> 13.4GB VRAM Footprint.
> 100% Offline Edge Deployment.

AUDIO: Five models working in perfect unison. A total VRAM footprint of just 13.4 gigabytes. Zero cloud dependencies. No patient data ever leaves the clinic.

Visual: Sam speaking directly to the camera, dead serious, unblinking.

AUDIO: I originally wrote 62,000 lines of memory code to fix my own failing recall. Today, that algorithm is fine-tuning MedGemma. Tomorrow, we use it to make sure the next generation of doctors never misses a shadow on an X-Ray again.
